#!/usr/bin/env python3
"""
Pre-Quest Example 3: Different Types of Embeddings
ComparaciÃ³n de diferentes tipos de embeddings y sus casos de uso
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

def ejemplo_bag_of_words():
    """
    ğŸ¯ Objetivo: Entender Bag of Words (BoW)
    """
    print("ğŸ“Š EJEMPLO 1: Bag of Words (BoW)")
    print("=" * 60)
    
    # Documentos de ejemplo
    documentos = [
        "el gato come pescado",
        "el perro come carne",
        "gato y perro son animales",
        "pescado y carne son comida"
    ]
    
    print("ğŸ“ Documentos:")
    for i, doc in enumerate(documentos):
        print(f"   {i+1}. '{doc}'")
    
    # Crear vectorizer BoW
    vectorizer = CountVectorizer()
    bow_matrix = vectorizer.fit_transform(documentos)
    
    # Mostrar vocabulario
    vocabulario = vectorizer.get_feature_names_out()
    print(f"\nğŸ“š Vocabulario: {list(vocabulario)}")
    
    # Mostrar matriz BoW
    print("\nğŸ”¢ Matriz Bag of Words:")
    print("   Filas = documentos, Columnas = palabras")
    print("   Valores = frecuencia de cada palabra")
    
    bow_array = bow_matrix.toarray()
    
    # Encabezados
    print("   " + " ".join([f"{word:8s}" for word in vocabulario]))
    
    for i, fila in enumerate(bow_array):
        print(f"{i+1}: " + " ".join([f"{val:8d}" for val in fila]))
    
    # Calcular similitudes
    print("\nğŸ” Similitudes entre documentos:")
    similitudes = cosine_similarity(bow_array)
    
    for i in range(len(documentos)):
        for j in range(i+1, len(documentos)):
            print(f"   Doc {i+1} â†” Doc {j+1}: {similitudes[i, j]:.3f}")
    
    return bow_array, vocabulario

def ejemplo_tfidf():
    """
    ğŸ¯ Objetivo: Entender TF-IDF (Term Frequency - Inverse Document Frequency)
    """
    print("\n\nğŸ“ˆ EJEMPLO 2: TF-IDF")
    print("=" * 60)
    
    # Documentos donde algunas palabras son mÃ¡s comunes
    documentos = [
        "el gato come pescado fresco",
        "el perro come carne roja",
        "el gato y el perro son animales domÃ©sticos",
        "pescado fresco y carne roja son comida nutritiva",
        "el agua es importante para todos los animales"
    ]
    
    print("ğŸ“ Documentos:")
    for i, doc in enumerate(documentos):
        print(f"   {i+1}. '{doc}'")
    
    # Crear vectorizer TF-IDF
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(documentos)
    
    vocabulario = tfidf_vectorizer.get_feature_names_out()
    print(f"\nğŸ“š Vocabulario: {list(vocabulario)}")
    
    # Mostrar matriz TF-IDF (solo valores significativos)
    print("\nğŸ”¢ Matriz TF-IDF (valores > 0.1):")
    tfidf_array = tfidf_matrix.toarray()
    
    for i, fila in enumerate(tfidf_array):
        print(f"\nDoc {i+1}:")
        for j, val in enumerate(fila):
            if val > 0.1:
                print(f"   {vocabulario[j]}: {val:.3f}")
    
    # Explicar TF-IDF
    print("\nğŸ’¡ Â¿QuÃ© significa TF-IDF?")
    print("   â€¢ TF (Term Frequency): Frecuencia del tÃ©rmino en el documento")
    print("   â€¢ IDF (Inverse Document Frequency): Rareza del tÃ©rmino en la colecciÃ³n")
    print("   â€¢ TF-IDF = TF Ã— IDF")
    print("   â€¢ Palabras comunes (como 'el') tienen IDF bajo")
    print("   â€¢ Palabras raras (como 'nutritiva') tienen IDF alto")
    
    return tfidf_array, vocabulario

def ejemplo_word2vec_simulado():
    """
    ğŸ¯ Objetivo: Simular embeddings densos como Word2Vec
    """
    print("\n\nğŸ§  EJEMPLO 3: Embeddings Densos (simulando Word2Vec)")
    print("=" * 60)
    
    # Simular embeddings densos de palabras
    # En la realidad, estos vienen de entrenar redes neuronales
    embeddings_palabras = {
        "gato": np.array([0.8, 0.1, 0.9, 0.2, 0.7]),
        "felino": np.array([0.7, 0.0, 0.8, 0.1, 0.8]),  # Similar a gato
        "perro": np.array([0.9, 0.0, 0.7, 0.3, 0.6]),
        "canino": np.array([0.8, 0.1, 0.6, 0.2, 0.7]),  # Similar a perro
        "pescado": np.array([0.1, 0.9, 0.2, 0.8, 0.1]),
        "carne": np.array([0.2, 0.8, 0.1, 0.9, 0.2]),
        "mesa": np.array([0.0, 0.0, 0.1, 0.0, 0.0]),     # Muy diferente
    }
    
    print("ğŸ”¢ Embeddings de palabras (vectores densos):")
    for palabra, embedding in embeddings_palabras.items():
        print(f"   {palabra:8s}: {embedding}")
    
    # Calcular similitudes
    print("\nğŸ” Similitudes entre palabras:")
    palabras = list(embeddings_palabras.keys())
    
    for i, palabra1 in enumerate(palabras):
        for j, palabra2 in enumerate(palabras[i+1:], i+1):
            emb1 = embeddings_palabras[palabra1]
            emb2 = embeddings_palabras[palabra2]
            
            similitud = cosine_similarity([emb1], [emb2])[0][0]
            print(f"   {palabra1:8s} â†” {palabra2:8s}: {similitud:.3f}")
    
    print("\nğŸ’¡ Observaciones:")
    print("   â€¢ 'gato' y 'felino' son muy similares (0.9+)")
    print("   â€¢ 'perro' y 'canino' son muy similares (0.9+)")
    print("   â€¢ 'pescado' y 'carne' son medianamente similares (comida)")
    print("   â€¢ 'mesa' es diferente a todo (objeto vs animales/comida)")

def ejemplo_embeddings_contextuales():
    """
    ğŸ¯ Objetivo: Simular embeddings contextuales
    """
    print("\n\nğŸ­ EJEMPLO 4: Embeddings Contextuales")
    print("=" * 60)
    
    # La misma palabra puede tener diferentes vectores segÃºn el contexto
    contextos = {
        "banco_financiero": {
            "frase": "Voy al banco a depositar dinero",
            "vector": np.array([0.8, 0.2, 0.9, 0.1, 0.7])
        },
        "banco_asiento": {
            "frase": "Me siento en el banco del parque",
            "vector": np.array([0.1, 0.9, 0.2, 0.8, 0.3])
        },
        "banco_peces": {
            "frase": "Vimos un banco de peces en el mar",
            "vector": np.array([0.3, 0.1, 0.2, 0.9, 0.8])
        }
    }
    
    print("ğŸ­ La palabra 'banco' en diferentes contextos:")
    
    for contexto, info in contextos.items():
        print(f"\nğŸ“ {contexto}:")
        print(f"   Frase: '{info['frase']}'")
        print(f"   Vector: {info['vector']}")
    
    # Calcular similitudes entre contextos
    print("\nğŸ” Similitudes entre diferentes usos de 'banco':")
    
    contextos_list = list(contextos.keys())
    for i, ctx1 in enumerate(contextos_list):
        for j, ctx2 in enumerate(contextos_list[i+1:], i+1):
            vec1 = contextos[ctx1]['vector']
            vec2 = contextos[ctx2]['vector']
            
            similitud = cosine_similarity([vec1], [vec2])[0][0]
            print(f"   {ctx1} â†” {ctx2}: {similitud:.3f}")
    
    print("\nğŸ’¡ Ventajas de embeddings contextuales:")
    print("   â€¢ La misma palabra puede tener vectores diferentes")
    print("   â€¢ El significado depende del contexto")
    print("   â€¢ Capturan polisemia (mÃºltiples significados)")
    print("   â€¢ Modelos como BERT, GPT usan este enfoque")

def comparar_todos_los_metodos():
    """
    ğŸ¯ Objetivo: Comparar todos los mÃ©todos de vectorizaciÃ³n
    """
    print("\n\nâš–ï¸ EJEMPLO 5: ComparaciÃ³n de MÃ©todos")
    print("=" * 60)
    
    # Frase de prueba
    frase_consulta = "animal domÃ©stico"
    documentos = [
        "gato animal domÃ©stico",
        "felino mascota casa",
        "perro animal leal", 
        "mesa mueble madera",
        "coche vehÃ­culo rÃ¡pido"
    ]
    
    print(f"ğŸ” Consulta: '{frase_consulta}'")
    print("ğŸ“š Documentos:")
    for i, doc in enumerate(documentos):
        print(f"   {i+1}. '{doc}'")
    
    # MÃ©todo 1: Bag of Words
    print("\nğŸ“Š 1. Bag of Words:")
    bow_vectorizer = CountVectorizer()
    bow_matrix = bow_vectorizer.fit_transform(documentos + [frase_consulta])
    bow_similitudes = cosine_similarity([bow_matrix[-1]], bow_matrix[:-1])[0]
    
    for i, sim in enumerate(bow_similitudes):
        print(f"   Doc {i+1}: {sim:.3f}")
    
    # MÃ©todo 2: TF-IDF
    print("\nğŸ“ˆ 2. TF-IDF:")
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(documentos + [frase_consulta])
    tfidf_similitudes = cosine_similarity([tfidf_matrix[-1]], tfidf_matrix[:-1])[0]
    
    for i, sim in enumerate(tfidf_similitudes):
        print(f"   Doc {i+1}: {sim:.3f}")
    
    # MÃ©todo 3: Embeddings densos (simulados)
    print("\nğŸ§  3. Embeddings Densos (simulados):")
    
    # Simular embeddings mÃ¡s inteligentes
    embeddings_docs = np.array([
        [0.9, 0.8, 0.1, 0.2, 0.9],  # gato animal domÃ©stico
        [0.8, 0.9, 0.0, 0.1, 0.8],  # felino mascota casa
        [0.7, 0.7, 0.2, 0.3, 0.8],  # perro animal leal
        [0.0, 0.1, 0.9, 0.8, 0.0],  # mesa mueble madera
        [0.1, 0.0, 0.8, 0.9, 0.1],  # coche vehÃ­culo rÃ¡pido
    ])
    
    embedding_consulta = np.array([0.8, 0.8, 0.1, 0.2, 0.9])  # animal domÃ©stico
    
    dense_similitudes = cosine_similarity([embedding_consulta], embeddings_docs)[0]
    
    for i, sim in enumerate(dense_similitudes):
        print(f"   Doc {i+1}: {sim:.3f}")
    
    # Resumen
    print("\nğŸ† Mejor match por mÃ©todo:")
    print(f"   BoW: Doc {np.argmax(bow_similitudes) + 1} ({np.max(bow_similitudes):.3f})")
    print(f"   TF-IDF: Doc {np.argmax(tfidf_similitudes) + 1} ({np.max(tfidf_similitudes):.3f})")
    print(f"   Dense: Doc {np.argmax(dense_similitudes) + 1} ({np.max(dense_similitudes):.3f})")

def main():
    """
    Ejecutar todos los ejemplos
    """
    print("ğŸ”¬ PRE-QUEST: Tipos de Embeddings")
    print("ğŸ¯ ComparaciÃ³n de diferentes mÃ©todos de vectorizaciÃ³n")
    print("=" * 80)
    
    # Ejecutar ejemplos
    ejemplo_bag_of_words()
    ejemplo_tfidf()
    ejemplo_word2vec_simulado()
    ejemplo_embeddings_contextuales()
    comparar_todos_los_metodos()
    
    print("\n\nğŸ‰ Â¡ComparaciÃ³n de embeddings completada!")
    print("ğŸ’¡ Conceptos clave aprendidos:")
    print("   â€¢ Bag of Words: Conteo simple de palabras")
    print("   â€¢ TF-IDF: Pondera importancia de tÃ©rminos")
    print("   â€¢ Word2Vec: Embeddings densos entrenados")
    print("   â€¢ Embeddings contextuales: Vectores dependientes del contexto")
    print("   â€¢ Trade-offs entre interpretabilidad y capacidad semÃ¡ntica")
    
    print("\nğŸ“Š Resumen de mÃ©todos:")
    print("   â€¢ Sparse (BoW, TF-IDF): Interpretables, dimensiÃ³n alta")
    print("   â€¢ Dense (Word2Vec, BERT): SemÃ¡nticos, dimensiÃ³n baja")
    print("   â€¢ Contextuales: Capturan polisemia y contexto")
    
    print("\nğŸ® Â¡Ahora entiendes las bases para todos los Quests!")

if __name__ == "__main__":
    main()